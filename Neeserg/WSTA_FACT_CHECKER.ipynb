{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xapian\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "DROPBOX_PATH = \"/Users/neesergparajuli/Dropbox/\"\n",
    "dev_SET = DROPBOX_PATH+\"Webtext/Data/devset.json\"\n",
    "predictor = Predictor.from_path(DROPBOX_PATH+\"Webtext/WSTA_fact_checker/allen3/model.tar.gz\")\n",
    "DATABASE = DROPBOX_PATH+\"Webtext/Data/XxapianDatabase3\"\n",
    "db = xapian.Database(DATABASE)\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing queries and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(fact):\n",
    "    sent = nlp(fact)\n",
    "    query = []\n",
    "    sentence = []\n",
    "    title = []\n",
    "    for ent in sent.ents:\n",
    "        query.append(\"text:\" + '\"' + ent.text + '\"')\n",
    "        query.append(\"title:\" + '\"' + ent.text + '\"')\n",
    "        title.append(ent.text)\n",
    "    for chunk in sent.noun_chunks:\n",
    "        if chunk.root.dep_ == 'nsubj':\n",
    "            title.append(chunk.text)\n",
    "    for token in sent:\n",
    "        if token.tag_ == \"NN\" or token.pos_ == \"PROPN\" or token.tag_ == \"VBG\":\n",
    "            query.append(\"text:\" + '\"' + token.text + '\"')\n",
    "            sentence.append(token.text)\n",
    "#         if token.pos_ == \"PROPN\":\n",
    "#             title.append(token.text)\n",
    "            \n",
    "    return (query,sentence, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the databases and process results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_id(terms):\n",
    "    for term in terms:\n",
    "        if \"_~s~_\" in term:\n",
    "            ID = term.split(\"_~s~_\" )\n",
    "            docId = ID[0][1:]\n",
    "            return docId\n",
    "    \n",
    "    print(\"wtdf\")\n",
    "    return \"wtf\"\n",
    "\n",
    "def my_own_queryparser(fact):\n",
    "    new_fact = ''\n",
    "    for f in fact:\n",
    "        new_fact += f + ' '\n",
    "    return new_fact\n",
    "\n",
    "def get_sentences(docs):\n",
    "    doc_dict = {}\n",
    "    for doc in docs:\n",
    "        sent = doc[1].split('.\\n')\n",
    "        doc_dict[doc[0]] = {i: sent[i] for i in range(len(sent)-1) }\n",
    "    return doc_dict\n",
    "def sort_doc(title,docs, processed, t_processed):\n",
    "    title = \" \".join(title.split('_'))\n",
    "    s= 0\n",
    "    for fact in t_processed:\n",
    "        if fact in title:\n",
    "            s+= 20*(len(fact)/len(title))\n",
    "    counts = Counter()\n",
    "    i = 1\n",
    "    for key in docs:\n",
    "        i += 1\n",
    "        for fact in (processed+t_processed):\n",
    "            if counts[key] == 0:\n",
    "                counts[key] = 1\n",
    "            if fact in docs[key]:\n",
    "                counts[key] +=1\n",
    "            else:\n",
    "                counts[key] +=0\n",
    "    \n",
    "    return (s + sum(counts.values())/np.sqrt(i), counts.most_common())\n",
    "\n",
    "def get_best_sentences(querys ,docs, n_docs = 5, n_sentences = 20):\n",
    "    best_docs = []\n",
    "    for doc in docs:\n",
    "        (doc_scores, sentence_score) = sort_doc(doc,docs[doc],querys[1], querys[2])\n",
    "        scores = (doc_scores, doc, sentence_score)\n",
    "        best_docs.append(scores)\n",
    "    best_docs.sort(reverse =True)\n",
    "    best_docs = best_docs[:n_docs]\n",
    "    best_sent = Counter()\n",
    "    for best in best_docs:\n",
    "        for sent in best[2]:\n",
    "            best_sent[(best[1], sent[0])] = sent[1]*best[0]\n",
    "    return best_sent.most_common(n_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(querys, k_matches):\n",
    "    fact = querys[0]\n",
    "    queryparser = xapian.QueryParser()\n",
    "    queryparser.set_stemming_strategy(xapian.QueryParser.STEM_NONE)\n",
    "    queryparser.add_prefix(\"text\", \"\")\n",
    "    queryparser.add_prefix(\"title\", \"S\")\n",
    "    query = queryparser.parse_query(my_own_queryparser(fact))\n",
    "    enquire = xapian.Enquire(db)\n",
    "    enquire.set_query(query)\n",
    "    results = []\n",
    "    for match in enquire.get_mset(0,k_matches):\n",
    "        terms = match.document.termlist()\n",
    "        unid = get_unique_id([term.term.decode(\"utf8\") for term in terms])\n",
    "        results.append((unid, match.document.get_data().decode(\"utf8\")))\n",
    "    docs = get_sentences(results)\n",
    "    best_matches = get_best_sentences(querys, docs)\n",
    "    final_result = []\n",
    "    for match in best_matches:\n",
    "        doc_id, sent_id = match[0]\n",
    "        final_result.append(((doc_id, sent_id), docs[doc_id][sent_id]))\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dev_SET) as file:\n",
    "    data_ = json.load(file)\n",
    "data ={}\n",
    "with open(\"practice_dev.json\", \"w\") as file:\n",
    "    for ob in data_:\n",
    "        if np.random.uniform()<0.05:\n",
    "            data[ob] = data_[ob]\n",
    "    json.dump(data, file, indent =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topN(fact, n_fact):\n",
    "    processed = preprocess(fact)\n",
    "    premises = get_docs(processed, n_fact)\n",
    "    facts_prob ={\"SUPPORTS\":[], \"REFUTES\":[]}\n",
    "    i=0\n",
    "    x = np.random.uniform()\n",
    "    for premise in premises:\n",
    "        probs = predictor.predict(premise = premise[1], hypothesis = fact)[\"label_probs\"]\n",
    "        (prob1, prob2) = (probs[1], probs[2] )\n",
    "        if i >8:\n",
    "            break\n",
    "        if(prob1>0.5):\n",
    "            facts_prob[\"SUPPORTS\"].append((prob1,premise[0]))\n",
    "        if(prob2>0.5):\n",
    "            facts_prob[\"REFUTES\"].append((prob2, premise[0]))\n",
    "        i +=1\n",
    "#     facts_prob[\"SUPPORTS\"].sort(reverse = True)\n",
    "#     facts_prob[\"REFUTES\"].sort(reverse = True)\n",
    "    return facts_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "empty = 0\n",
    "js_dict = {}\n",
    "for ob in data:\n",
    "    m+=1\n",
    "    fact = data[ob][\"claim\"]\n",
    "    premises = get_topN(fact, 100)\n",
    "    refutes = premises[\"REFUTES\"]\n",
    "    supports = premises[\"SUPPORTS\"]\n",
    "    if len(refutes) == len(supports):\n",
    "        js_dict[ob] = {\"claim\": fact, \"label\": \"NOT ENOUGH INFO\", \"evidence\": []}\n",
    "    elif len(refutes) > len(supports):\n",
    "        js_dict[ob] = {\"claim\": fact, \"label\": \"REFUTES\", \"evidence\": [ [ref[1][0], ref[1][1]] for ref in refutes ]}\n",
    "    else:\n",
    "        js_dict[ob] = {\"claim\": fact, \"label\": \"SUPPORTS\", \"evidence\": [ [sup[1][0], sup[1][1]] for sup in supports ]}\n",
    "with open(\"prediction_dev.json\",\"w\") as file:\n",
    "    json.dump(js_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Pharrell_Williams', 7), \"Williams owns a media venture that encompasses entertainment , music , fashion , and art called i am OTHER , a multimedia creative collective and record label that serves as an umbrella for all of Pharrell Williams ' endeavors , including Billionaire Boys Club \"), (('Pharrell_Williams_discography', 0), 'The discography of Pharrell Williams , an American recording artist and record producer , consists of two studio albums , two extended plays -LRB- EPs -RRB- , one mixtape , 46 singles -LRB- including 38 as a featured artist -RRB- and 40 guest appearances '), (('Pharrell_Williams', 0), 'Pharrell Lanscilo Williams -LRB- -LSB- fəˈɹ̠ɛːl -RSB- born April 5 , 1973 -RRB- is an American singer-songwriter , rapper , record producer , and film producer '), (('Happy_-LRB-Pharrell_Williams_song-RRB-', 0), \"`` Happy '' is a song written , produced , and performed by American singer and record producer Pharrell Williams , from the Despicable Me 2 soundtrack album \"), (('Girl_-LRB-Pharrell_Williams_album-RRB-', 0), 'Girl -LRB- stylized as G I R L -RRB- is the second studio album by American singer and record producer Pharrell Williams '), (('That_Girl_-LRB-Pharrell_Williams_song-RRB-', 0), \"`` That Girl '' is by a song by American singer and record producer Pharrell Williams \"), (('Pharrell_Williams', 1), 'Williams and Chad Hugo make up the record production duo The Neptunes , producing soul , hip hop and R&B music '), (('Pharrell_Williams', 6), 'As part of the Neptunes , Williams has produced numerous hit singles for various recording artists '), (('Pharrell_Williams', 8), 'Williams has earned ten Grammy Awards including two with the Neptunes '), (('That_Girl_-LRB-Pharrell_Williams_song-RRB-', 1), \"The song features guest vocals from Pharrell 's longtime collaborators Snoop Dogg and Uncle Charlie Wilson , and produced by Williams \"), (('Pharrell_Williams_discography', 1), \"On September 9 , 2005 , Williams performed the opening single from his first solo album In My Mind , `` Can I Have It Like That '' , featuring Gwen Stefani \"), (('Pharrell_Williams_discography', 6), \"In 2013 , Williams featured on Daft Punk 's fourth studio album Random Access Memories on the songs `` Get Lucky '' and `` Lose Yourself to Dance '' \"), (('Pharrell_Williams_discography', 7), \"In addition , Williams contributed to Azealia Banks ' upcoming debut studio album Broke with Expensive Taste , featuring on the song `` ATM Jam '' \"), (('Pharrell_Williams_discography', 8), \"Williams penned three new original songs , included alongside composer Heitor Pereira 's score , for the sequel Despicable Me 2 \"), (('Pharrell_Williams_discography', 11), \"In March 2013 , Williams appeared alongside T.I. on Robin Thicke 's hit single `` Blurred Lines '' \"), (('Pharrell_Williams_discography', 12), \"The song has been a worldwide hit , has peaked at number one on the Billboard Hot 100 , and has also reached number one in 13 more countries including the United Kingdom and Germany , making it Williams ' third Billboard Hot 100 number one single \"), (('Pharrell_Williams_discography', 14), \"By July 2013 , only 137 singles in UK chart history had achieved one million sales in the UK - that month Williams scored two million-sellers with `` Get Lucky '' and `` Blurred Lines '' \"), (('Pharrell_Williams_discography', 15), \"In November 2013 , Williams released the first 24-hour music video to his Despicable Me 2 collaboration song , `` Happy '' \"), (('Pharrell_Williams_discography', 17), \"In December 2013 , a press release from Columbia Records announced that Williams had signed a contract with the label and would be releasing an album in 2014 , featuring the single `` Happy '' from the Despicable Me 2 soundtrack \"), (('Pharrell_Williams_discography', 20), \"On March 4 , 2014 , Williams released his second studio album Girl , which includes the singles `` Happy '' , `` Marilyn Monroe '' , `` Come Get It Bae '' and `` Gust of Wind '' \")]\n"
     ]
    }
   ],
   "source": [
    "print(get_docs(preprocess(\"Pharrell Williams is not a musician.\"), 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
